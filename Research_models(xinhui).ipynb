{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Research_models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binglesley/Project-90106-G28/blob/master/Research_models(xinhui).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQRqdUQLjvOQ",
        "colab_type": "text"
      },
      "source": [
        "# Research Data-G28\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz0bcj9EjvOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXWpvTDQjvOc",
        "colab_type": "text"
      },
      "source": [
        "This part is loding data set. You just need to modify the filename for different data file. The latest data set is 'cleaned_6_Sep.csv'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTLW2TOtjvOd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename):\n",
        "    filepath = os.path.join(os.getcwd(), filename)\n",
        "    data = pd.read_csv(filepath, index_col='Id')\n",
        "    data['Is_External__c'] = data['Is_External__c'].fillna('Internal')\n",
        "    try:\n",
        "      data = data.drop('Status_Reason__c',axis = 1)\n",
        "    except Exception as e:\n",
        "      print(\"No this features\")\n",
        "    #d = data[data['StageName'].isin(['Closed Lost','Closed Won'])]\n",
        "\n",
        "    return data"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lot0h_zfjvOl",
        "colab_type": "text"
      },
      "source": [
        "This part is help you to delete features which is missing more than 40% and also factorize the catogorical data. It will renturn the training set, labels and also the labels' values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFfE-9b8jvOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Deleting(data):\n",
        "    for index,col in data.iteritems(): \n",
        "        null_proportion = cal_null(col)\n",
        "\n",
        "    if null_proportion >= 0.4: # if missing more than 40% then remove this feature\n",
        "        data = data.drop(index, axis=1)\n",
        "        \n",
        "    data = data.dropna()\n",
        "\n",
        "    for index, col in data.iteritems():# finding if it is numerical fetures\n",
        "        types = type_(col)\n",
        "        if types == 'catogerical':\n",
        "            if index == 'StageName':\n",
        "                labels_corres = pd.factorize(data[index])[1]\n",
        "                data[index] = pd.factorize(data[index])[0].astype(np.uint16)\n",
        "            else:\n",
        "                data[index] = pd.factorize(data[index])[0].astype(np.uint16)\n",
        "\n",
        "    x = data\n",
        "    Labels = x['StageName']\n",
        "    Trains = x.drop('StageName',axis=1)\n",
        "    #Trains = Trains.drop('Status_Reason__c',axis = 1)\n",
        "    names = Trains.columns\n",
        "\n",
        "\n",
        "    return Trains,Labels,names # 'neams' is the list of features' name \n",
        "\n",
        "\n",
        "def cal_null(col):\n",
        "    is_null = col[col.isnull()]\n",
        "    percentage = len(is_null)/len(col)\n",
        "    return percentage\n",
        "\n",
        "def type_(col):\n",
        "    not_null = col[col.notnull()]\n",
        "    element = str(not_null[0])\n",
        "    if element.isdigit():\n",
        "        return 'number'\n",
        "    else:\n",
        "        return 'catogerical'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNysO69vjvOw",
        "colab_type": "text"
      },
      "source": [
        "This function is designed for deal with the original data set and return certain form of data set which could use to imputation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIK6_O4sjvO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deal_data(raw_data):\n",
        "    missing_set = {}\n",
        "    no_missing = {}\n",
        "    index_missing = {}\n",
        "    index_no_missing = {}\n",
        "    labels = {}\n",
        "    missing_names = []\n",
        "    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "    \n",
        "\n",
        "    for index, col in raw_data.iteritems():\n",
        "      if raw_data[index].isnull().any():\n",
        "        if len(col[col.isnull()])/len(col) >= 0.4:  # if more than 40% missing, would use the most frequenct records to impute.\n",
        "            raw_data[index] = imp.fit_transform(raw_data[index].values.reshape(-1, 1))\n",
        "        else:\n",
        "            index_missing[index] = col[col.isnull()].index # each missing features' index\n",
        "            index_no_missing[index] = col[col.notnull()].index\n",
        "            missing_names.append(index)\n",
        "\n",
        "    for index,col in raw_data.iteritems():\n",
        "        if str(col[col.notnull()][0]).isdigit() == False:\n",
        "            if index == 'StageName':\n",
        "                labels_corres = pd.factorize(raw_data[index])[1]\n",
        "            raw_data[index] = pd.factorize(raw_data[index])[0].astype(np.uint16)\n",
        "\n",
        "    for i in missing_names:\n",
        "        labels[i] = raw_data.loc[index_no_missing[i],i]\n",
        "\n",
        "\n",
        "    raw_data = raw_data.dropna(axis = 1)\n",
        "    \n",
        "\n",
        "    for key,value in index_missing.items():\n",
        "        missing_set[key] = raw_data.loc[value]\n",
        "        no_values = index_no_missing[key]\n",
        "        no_missing[key] = raw_data.loc[no_values]\n",
        "\n",
        "    return no_missing,missing_set,index_no_missing,index_missing,labels,missing_names \n",
        "    # 'missing_names' is the list of fratures' name which contain missing values \n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdaKvy87jvO9",
        "colab_type": "text"
      },
      "source": [
        "This part is imputing with random-forest method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EYSuKJ_jvO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def impute(no_missing,missing_set,index_missing,labels,names,raw_data):\n",
        "    predict = {}\n",
        "    features_name = []\n",
        "\n",
        "    for feature in names:\n",
        "        label = labels[feature]\n",
        "        train_set = no_missing[feature]\n",
        "        missing = missing_set[feature]\n",
        "        predict[feature] = random_forest(train_set,label,missing)\n",
        "\n",
        "    for key,items in predict.items():\n",
        "        c = 0\n",
        "        for i in index_missing[key]:\n",
        "            raw_data.loc[i,key] = items[c]\n",
        "            c += 1\n",
        "    train = raw_data.drop('StageName',axis=1)\n",
        "    features_name = [index for index,col in train.iteritems()] #'feature_name' is the list of features' name\n",
        "    y_label = raw_data['StageName']\n",
        "\n",
        "    return train,y_label,features_name\n",
        "\n",
        "\n",
        "def random_forest(train,label,missing):\n",
        "  random_f = RandomForestRegressor()\n",
        "  random_f.fit(train,label)\n",
        "  y = random_f.predict(missing)\n",
        "  return y\n",
        "\n",
        "def importances(train,label):\n",
        "  random_f = RandomForestRegressor()\n",
        "  random_f.fit(train,label)\n",
        "  importance = (sorted(zip(map(lambda x: round(x, 4), random_f.feature_importances_),features_name),\n",
        "             reverse=True)) # the importance ranking of features \n",
        "    \n",
        "  return importance"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcALOn-jvPC",
        "colab_type": "text"
      },
      "source": [
        "This part catains two training model, SVM and Decision tree. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEtdGqzejvPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svc(train_set,label_set,test_set,ground_truth):\n",
        "    svm_clf = LinearSVC(max_iter=10000)\n",
        "    svm_clf.fit(train_set, label_set)\n",
        "    y = svm_clf.predict(test_set)\n",
        "    p = precision_score(y,ground_truth,average= None)\n",
        "    r = recall_score(y,ground_truth,average=None)\n",
        "    f = f1_score(y,ground_truth,average=None)\n",
        "    print('svm_precision for each labels:',p,'\\n')\n",
        "    print('svm_recall for each labels:',r,'\\n')\n",
        "    print('svm_f1 for each labels:',f,'\\n')\n",
        "\n",
        "    # precision score, recall, F1 score\n",
        "\n",
        "def decision_tree(train_set,label_set,test_set,ground_truth):\n",
        "    tree_clf = DecisionTreeClassifier()\n",
        "    tree_clf.fit(train_set,label_set)\n",
        "    y = tree_clf.predict(test_set)\n",
        "    p = precision_score(y, ground_truth, average=None)\n",
        "    r = recall_score(y, ground_truth, average=None)\n",
        "    f = f1_score(y, ground_truth, average=None)\n",
        "    print('tree_precision for each labels:',p,'\\n')\n",
        "    print('tree_recall for each labels:',r,'\\n')\n",
        "    print('tree_f1 for each labels:',f,'\\n')"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUIjGlvwjvPR",
        "colab_type": "text"
      },
      "source": [
        "This part build an NN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOOMGL2jvPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_build(shapes):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_dim=shapes))\n",
        "    model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfWSBU5R9DOB",
        "colab_type": "text"
      },
      "source": [
        "This part is cross-validation for models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYOHbsNV842G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_folds(X_train,y_train,models):\n",
        "  scores  = cross_val_score(estimator = models,X = X_train,\n",
        "                      y = y_train,cv = 10,n_jobs = 1)\n",
        "    print('CV accuracy scores:\\n\\n %s \\n' % scores)\n",
        "    print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91KnqNgijvPb",
        "colab_type": "text"
      },
      "source": [
        "To check the accuracy of no cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igim-Y9AjvPd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "ded4ca11-c2c3-4740-d021-96280c673c7e"
      },
      "source": [
        "raw_data = load_data('cleaned_6_Sep.csv')\n",
        "no_missing, missing_set, index_no_missing, index_missing, labels, missing_names = deal_data(raw_data)\n",
        "X_set,y,features_name = impute(no_missing, missing_set, index_missing, labels, missing_names, raw_data)\n",
        "shapes = X_set.shape[1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_set, y, train_size=0.8, random_state=1)\n",
        "print('Features importance:',importances(X_train,y_train))\n",
        "svc(X_train,y_train,X_test,y_test)\n",
        "decision_tree(X_train,y_train,X_test,y_test)\n",
        "X_train1,X_dev,y_train1,y_dev = train_test_split(X_train,y_train,train_size=0.8,random_state=1)\n",
        "model = nn_build(shapes)\n",
        "model.compile(optimizer='adam',\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(X_train1, y_train1, epochs=10, verbose=True, validation_data=(X_dev, y_dev), batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"NN Testing Accuracy:  {:.4f}\".format(accuracy))\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features importance: [(0.2524, 'CreatedDate'), (0.1132, 'Lead_Faculty__c'), (0.0997, 'CloseDate'), (0.0983, 'AccountId'), (0.0676, 'CloseMonth'), (0.0627, 'CreatedMonth'), (0.0511, 'Lead_School__c'), (0.0442, 'Business_Type__c'), (0.042, 'RecordType.Name'), (0.0344, 'Industry'), (0.0299, 'Industry_Sub_Type__c'), (0.0251, ' Time_length'), (0.0235, 'CloseYear'), (0.0194, 'CreatedYear'), (0.0188, 'RICE_Supported__c'), (0.0093, 'ParentId'), (0.0083, 'Parent_Opportunity__c'), (0.0001, 'RecordType.Name.1'), (0.0, 'Is_External__c'), (0.0, 'Actual_Close_Date__c')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "svm_precision for each labels: [0.55667145 0.82800983] \n",
            "\n",
            "svm_recall for each labels: [0.84716157 0.52167183] \n",
            "\n",
            "svm_f1 for each labels: [0.67186147 0.64007597] \n",
            "\n",
            "tree_precision for each labels: [0.80774749 0.67076167] \n",
            "\n",
            "tree_recall for each labels: [0.80774749 0.67076167] \n",
            "\n",
            "tree_f1 for each labels: [0.80774749 0.67076167] \n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 300)               6300      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 36,501\n",
            "Trainable params: 36,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 20.6824 - accuracy: 0.5908 - val_loss: 12.9893 - val_accuracy: 0.5504\n",
            "Epoch 2/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 6.4996 - accuracy: 0.5999 - val_loss: 2.2762 - val_accuracy: 0.6535\n",
            "Epoch 3/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 6.1519 - accuracy: 0.5942 - val_loss: 2.4049 - val_accuracy: 0.6738\n",
            "Epoch 4/10\n",
            "353/353 [==============================] - 1s 2ms/step - loss: 4.2427 - accuracy: 0.5939 - val_loss: 1.7297 - val_accuracy: 0.6365\n",
            "Epoch 5/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 4.4582 - accuracy: 0.6039 - val_loss: 1.8935 - val_accuracy: 0.6138\n",
            "Epoch 6/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 3.0515 - accuracy: 0.6075 - val_loss: 3.2762 - val_accuracy: 0.5629\n",
            "Epoch 7/10\n",
            "353/353 [==============================] - 1s 2ms/step - loss: 3.2059 - accuracy: 0.6039 - val_loss: 1.2988 - val_accuracy: 0.6648\n",
            "Epoch 8/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 2.1515 - accuracy: 0.6084 - val_loss: 2.1750 - val_accuracy: 0.6365\n",
            "Epoch 9/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 2.6012 - accuracy: 0.5985 - val_loss: 1.7652 - val_accuracy: 0.6444\n",
            "Epoch 10/10\n",
            "353/353 [==============================] - 1s 3ms/step - loss: 2.0122 - accuracy: 0.6061 - val_loss: 1.3625 - val_accuracy: 0.6217\n",
            "Testing Accuracy:  0.6359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NuSSv_G9QLw",
        "colab_type": "text"
      },
      "source": [
        "To check accuracy of cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GuEfBvm9UyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "2e6a9f3b-5021-409c-fece-7293fa0d99b2"
      },
      "source": [
        "# For SVM model\n",
        "svm = svm_clf = LinearSVC(max_iter=10000)\n",
        "k_folds(X_set,y,svm)\n",
        "# For decision tree\n",
        "trees = DecisionTreeClassifier()\n",
        "k_folds(X_set,y,trees)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CV accuracy scores:\n",
            "\n",
            " [0.67753623 0.90217391 0.88949275 0.61413043 0.61413043 0.61231884\n",
            " 0.38475499 0.66969147 0.64791289 0.70961887] \n",
            "\n",
            "CV accuracy: 0.672 +/- 0.140\n",
            "CV accuracy scores:\n",
            "\n",
            " [0.60326087 0.45471014 0.03804348 0.04710145 0.55615942 0.46557971\n",
            " 0.00181488 0.00181488 0.07259528 0.38656987] \n",
            "\n",
            "CV accuracy: 0.263 +/- 0.238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Wbe_ftjvPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}