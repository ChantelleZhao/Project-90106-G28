---
title: "Lead_Scoring"
author: "Ziyan Zhao"
date: "31/08/2020"
output: pdf_document
header-includes: \usepackage{color,amsmath}
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lattice)
library(ggplot2)
library(caret)
```

### Attributes:
1: Id	
2: StageName (factor)	
3: Status_Reason__c	
4: RecordType.Name	
5: RICE_Supported__c	
6: CreatedDate
\newline 7: AccountId	
8: Lead_Faculty__c
\newline 9: Parent_Opportunity__c (factor) 
\newline 10: RecordType.Name.1 
\newline 11: Industry 
\newline 12: Business_Type__c 
\newline 13: Is_External__c 
\newline 14: ParentId (factor) 
\newline 15: RecordType


```{r,echo=T, eval=T,cache=T, message=F,warning=F}
# Reading Data In
data <- read.csv("/Users/ChantelleChiu/Documents/GitHub/Project-90106-G28/Chantelle/cleaned_2.csv",
                 header = TRUE, stringsAsFactors = T)
names(data) <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10",
                 "X11", "X12",
                 "X13", "X14", "X15") 

summary(data)
```


```{r,echo=T, eval=T,cache=T, message=F,warning=F}
mod1 <- glm(factor(X2)~X3+X4+X5+X6+X8+factor(X9)+X10+X11+X12+X13+factor(X14)+X15, 
            family = "binomial", data = data)
```


```{r,echo=T, eval=T,cache=T, message=F,warning=F}
# 10-fold cross_validation
train_control <- trainControl(method = "cv", number = 10)

# train the model on data set
model <- train(factor(X2)~X3+X4+X5+X6+X8+factor(X9)+X10+X11+X12+X13+factor(X14)+X15,
               data = data,
               trControl = train_control,
               method = "glm",
               family="binomial")

# print cv scores
print(model)

summary(model)
```
## Conclusion:
No attributes are significant (p-value < 0.05)

### Next step:
Remove attribute X3: "Status_Reason__c"

```{r,echo=T, eval=T,cache=T, message=F,warning=F}
mod2 <- glm(factor(X2)~X4+X5+X6+X8+factor(X9)+X10+X11+X12+X13+factor(X14)+X15, 
            family = "binomial", data = data)
```

```{r,echo=T, eval=T,cache=T, message=F,warning=F}
# 10-fold cross_validation
train_control <- trainControl(method = "cv", number = 10)

# train the model on data set
model <- train(factor(X2)~X4+X5+X6+X8+factor(X9)+X10+X11+X12+X13+factor(X14)+X15,
               data = data,
               trControl = train_control,
               method = "glm",
               family="binomial")
```

```{r,echo=T, eval=T,cache=T, message=F,warning=F}
summary(model)
```
## Conclusion:
### Significant Attributes:

### Very significant (p-value < 0.001): 
  X4Competitive Bid
  \newline X4Parent Grant
  \newline X5RIC RE&D and BD&I
  \newline X5RIC-BD&I
  \newline X6
  \newline X80012e000002ZFZYAA4
  \newline X80012e000002ZGfbAAG         
  X80012e000002ZLg7AAG         
  X80012e000002ZLgIAAW         
  X80012e000002ZLgJAAW         
  X80012e000002ZmnxAAC         
  X80012e000002ZmnzAAC          
  X80012e000002Zmo0AAC         
  X80012e000002ZmOUAA0         
  X80012e000002ZmOZAA0         
  X80012e000002ZNYOAA4 
  X80012e000002Zt1RAAS
  X13missing
  
### Significant (0,001 < p-value < 0.01)
  X4Technology Transfer
  \newline X5RIC-RE&D
  \newline factor(X9)1
  \newline X12Multinational / Other Large Corporate
  \newline X12PFRO (Publicly-Funded Research Organisation)
  
### Somehow significant (0.01 < p-value < 0.05)
  X4Panel
  \newline X4Philanthropic
  \newline X4Sponsorship
  
### 0.05 < p-value < 0.1 (keep or not?)
  factor(X14)1

```{r,echo=T, eval=T,cache=T, message=F,warning=F}
print(model)
```

from website
https://machinelearningmastery.com/machine-learning-evaluation-metrics-in-r/

Accuracy is the percentage of correctly classifies instances out of all instances. It is more useful on a binary classification than multi-class classification problems because it can be less clear exactly how the accuracy breaks down across those classes.

Kappa or Cohen’s Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0).

Suggested guidelines for Kappa:
\newline values <= 0 as indicating no agreement 
\newline 0.01–0.20 as none to slight
\newline 0.21–0.40 as fair, 0.41– 0.60 as moderate,
\newline 0.61–0.80 as substantial
\newline 0.81–1.00 as almost perfect agreement.